{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":82480,"databundleVersionId":9074971,"sourceType":"competition"}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom glob import glob\nfrom natsort import os_sorted\nfrom tqdm import tqdm\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-12T11:59:58.748019Z","iopub.status.idle":"2024-07-12T11:59:58.748361Z","shell.execute_reply.started":"2024-07-12T11:59:58.748197Z","shell.execute_reply":"2024-07-12T11:59:58.748212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install natsort","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-12T09:31:19.385335Z","iopub.execute_input":"2024-07-12T09:31:19.385693Z","iopub.status.idle":"2024-07-12T09:31:32.576412Z","shell.execute_reply.started":"2024-07-12T09:31:19.385662Z","shell.execute_reply":"2024-07-12T09:31:32.575231Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting natsort\n  Downloading natsort-8.4.0-py3-none-any.whl.metadata (21 kB)\nDownloading natsort-8.4.0-py3-none-any.whl (38 kB)\nInstalling collected packages: natsort\nSuccessfully installed natsort-8.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"'''input_dir = \"/kaggle/input/human-activity-recognition-har/HAR\"\ntrain_dir = input_dir + \"/train\"\ntest_dir  = input_dir + \"/test\"\ndf_paths  = glob(os.path.join(train_dir,'*','*.csv'))'''","metadata":{"execution":{"iopub.status.busy":"2024-07-12T09:51:08.618777Z","iopub.execute_input":"2024-07-12T09:51:08.619134Z","iopub.status.idle":"2024-07-12T09:51:08.629873Z","shell.execute_reply.started":"2024-07-12T09:51:08.619103Z","shell.execute_reply":"2024-07-12T09:51:08.629141Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"'''concatdf = []\nfor activity_dir in os.listdir(train_dir):\n    if activity_dir[0] == '.':\n        continue\n    print(activity_dir)\n    for csv_file in os.listdir(os.path.join(train_dir,activity_dir)):\n        print(os.path.join(train_dir, activity_dir, csv_file))\n        df = pd.read_csv(os.path.join(train_dir, activity_dir, csv_file))\n        df['label'] = activity_dir\n        print(csv_file,df.shape[0],\"->\",200*(df.shape[0]//200))\n        concatdf.append(df[:200*(df.shape[0]//200)])\n        # break\n    # break\nframe = pd.concat(concatdf)'''","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-12T09:46:27.316627Z","iopub.execute_input":"2024-07-12T09:46:27.316980Z","iopub.status.idle":"2024-07-12T09:46:29.779588Z","shell.execute_reply.started":"2024-07-12T09:46:27.316951Z","shell.execute_reply":"2024-07-12T09:46:29.778568Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Sitting\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/18.csv\n18.csv 1467 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/20.csv\n20.csv 15644 -> 15600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/24.csv\n24.csv 690 -> 600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/30.csv\n30.csv 1559 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/16.csv\n16.csv 2984 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/3.csv\n3.csv 1609 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/19.csv\n19.csv 2534 -> 2400\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/5.csv\n5.csv 1664 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/35.csv\n35.csv 1599 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/13.csv\n13.csv 1179 -> 1000\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/4.csv\n4.csv 1257 -> 1200\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/8.csv\n8.csv 2699 -> 2600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/12.csv\n12.csv 2289 -> 2200\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/7.csv\n7.csv 2529 -> 2400\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/6.csv\n6.csv 1679 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/21.csv\n21.csv 1609 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Sitting/27.csv\n27.csv 2099 -> 2000\nWalking\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/18.csv\n18.csv 12558 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/20.csv\n20.csv 13134 -> 13000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/1.csv\n1.csv 12861 -> 12800\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/24.csv\n24.csv 6256 -> 6200\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/11.csv\n11.csv 12139 -> 12000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/30.csv\n30.csv 12579 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/17.csv\n17.csv 9677 -> 9600\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/16.csv\n16.csv 12521 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/3.csv\n3.csv 12973 -> 12800\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/19.csv\n19.csv 17622 -> 17600\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/5.csv\n5.csv 12258 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/35.csv\n35.csv 7162 -> 7000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/2.csv\n2.csv 11739 -> 11600\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/26.csv\n26.csv 13210 -> 13200\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/28.csv\n28.csv 14169 -> 14000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/13.csv\n13.csv 13047 -> 13000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/23.csv\n23.csv 6589 -> 6400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/14.csv\n14.csv 13859 -> 13800\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/4.csv\n4.csv 6079 -> 6000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/22.csv\n22.csv 7029 -> 7000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/8.csv\n8.csv 17108 -> 17000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/12.csv\n12.csv 10798 -> 10600\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/9.csv\n9.csv 12923 -> 12800\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/7.csv\n7.csv 11033 -> 11000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/15.csv\n15.csv 11529 -> 11400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/6.csv\n6.csv 12399 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/10.csv\n10.csv 13048 -> 13000\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/25.csv\n25.csv 6979 -> 6800\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/21.csv\n21.csv 12498 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Walking/27.csv\n27.csv 12476 -> 12400\nDownstairs\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/18.csv\n18.csv 2415 -> 2400\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/20.csv\n20.csv 4673 -> 4600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/1.csv\n1.csv 2941 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/24.csv\n24.csv 2929 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/11.csv\n11.csv 2674 -> 2600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/30.csv\n30.csv 3872 -> 3800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/17.csv\n17.csv 3767 -> 3600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/16.csv\n16.csv 1575 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/3.csv\n3.csv 3326 -> 3200\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/19.csv\n19.csv 2614 -> 2600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/5.csv\n5.csv 3281 -> 3200\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/26.csv\n26.csv 3837 -> 3800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/28.csv\n28.csv 2997 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/13.csv\n13.csv 4241 -> 4200\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/23.csv\n23.csv 1939 -> 1800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/14.csv\n14.csv 2875 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/4.csv\n4.csv 1763 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/22.csv\n22.csv 3627 -> 3600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/8.csv\n8.csv 3346 -> 3200\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/12.csv\n12.csv 2870 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/7.csv\n7.csv 2257 -> 2200\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/15.csv\n15.csv 1762 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/6.csv\n6.csv 1433 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/10.csv\n10.csv 3795 -> 3600\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/21.csv\n21.csv 4036 -> 4000\n/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs/27.csv\n27.csv 3460 -> 3400\nStanding\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/18.csv\n18.csv 1954 -> 1800\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/20.csv\n20.csv 5389 -> 5200\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/24.csv\n24.csv 544 -> 400\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/30.csv\n30.csv 3099 -> 3000\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/16.csv\n16.csv 1979 -> 1800\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/3.csv\n3.csv 2824 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/19.csv\n19.csv 2132 -> 2000\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/5.csv\n5.csv 1515 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/35.csv\n35.csv 1069 -> 1000\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/28.csv\n28.csv 1300 -> 1200\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/13.csv\n13.csv 1659 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/8.csv\n8.csv 3269 -> 3200\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/12.csv\n12.csv 1670 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/7.csv\n7.csv 2364 -> 2200\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/6.csv\n6.csv 709 -> 600\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/10.csv\n10.csv 1660 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/21.csv\n21.csv 2859 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Standing/27.csv\n27.csv 1630 -> 1600\nUpstairs\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/18.csv\n18.csv 2425 -> 2400\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/20.csv\n20.csv 4844 -> 4800\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/1.csv\n1.csv 3120 -> 3000\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/24.csv\n24.csv 3039 -> 3000\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/11.csv\n11.csv 4392 -> 4200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/30.csv\n30.csv 4226 -> 4200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/17.csv\n17.csv 5689 -> 5600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/16.csv\n16.csv 1411 -> 1400\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/3.csv\n3.csv 3411 -> 3400\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/19.csv\n19.csv 4280 -> 4200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/5.csv\n5.csv 3387 -> 3200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/26.csv\n26.csv 3618 -> 3600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/28.csv\n28.csv 2892 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/13.csv\n13.csv 4638 -> 4600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/23.csv\n23.csv 4836 -> 4800\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/14.csv\n14.csv 8179 -> 8000\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/4.csv\n4.csv 1377 -> 1200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/22.csv\n22.csv 5430 -> 5400\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/8.csv\n8.csv 4453 -> 4400\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/12.csv\n12.csv 2654 -> 2600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/7.csv\n7.csv 3601 -> 3600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/15.csv\n15.csv 2064 -> 2000\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/6.csv\n6.csv 1666 -> 1600\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/10.csv\n10.csv 4296 -> 4200\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/21.csv\n21.csv 4841 -> 4800\n/kaggle/input/human-activity-recognition-har/HAR/train/Upstairs/27.csv\n27.csv 3255 -> 3200\nJogging\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/18.csv\n18.csv 11993 -> 11800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/20.csv\n20.csv 12948 -> 12800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/1.csv\n1.csv 11056 -> 11000\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/24.csv\n24.csv 12278 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/11.csv\n11.csv 12454 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/17.csv\n17.csv 2887 -> 2800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/3.csv\n3.csv 11018 -> 11000\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/19.csv\n19.csv 16201 -> 16200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/5.csv\n5.csv 6405 -> 6400\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/35.csv\n35.csv 12564 -> 12400\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/2.csv\n2.csv 11786 -> 11600\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/26.csv\n26.csv 11913 -> 11800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/13.csv\n13.csv 12329 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/23.csv\n23.csv 12309 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/14.csv\n14.csv 13279 -> 13200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/4.csv\n4.csv 895 -> 800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/22.csv\n22.csv 6224 -> 6200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/8.csv\n8.csv 10313 -> 10200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/12.csv\n12.csv 12360 -> 12200\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/7.csv\n7.csv 9183 -> 9000\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/15.csv\n15.csv 12800 -> 12800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/6.csv\n6.csv 11818 -> 11800\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/10.csv\n10.csv 12084 -> 12000\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/25.csv\n25.csv 6489 -> 6400\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/21.csv\n21.csv 9593 -> 9400\n/kaggle/input/human-activity-recognition-har/HAR/train/Jogging/27.csv\n27.csv 12039 -> 12000\n","output_type":"stream"}]},{"cell_type":"code","source":"'''nr = 200\nnew = []\nframe = frame\nfor i in tqdm(range(len(frame)//nr)):\n    x = frame[i*nr:(i+1)*nr]\n    a = x.groupby(['label'])[['x-axis','y-axis','z-axis']].mean()\n    a = a.rename(columns={'x-axis':'x-axismean','y-axis':'y-axismean','z-axis':'z-axismean'})\n    b = x.groupby(['label'])[['x-axis','y-axis','z-axis']].std()\n    b = b.rename(columns={'x-axis':'x-axisstd','y-axis':'y-axisstd','z-axis':'z-axisstd'})\n    ab = a.join(b)\n    new.append(ab)\ntrain_df = pd.concat(new)\ntrain_df'''","metadata":{"execution":{"iopub.status.busy":"2024-07-12T09:31:43.652924Z","iopub.execute_input":"2024-07-12T09:31:43.653239Z","iopub.status.idle":"2024-07-12T09:31:58.197200Z","shell.execute_reply.started":"2024-07-12T09:31:43.653211Z","shell.execute_reply":"2024-07-12T09:31:58.196336Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"100%|██████████| 4345/4345 [00:14<00:00, 306.50it/s]\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"         x-axismean  y-axismean  z-axismean  x-axisstd  y-axisstd  z-axisstd\nlabel                                                                       \nSitting     5.58130     1.89995    8.344250   0.121797   0.096139   0.104542\nSitting     5.47165     1.72050    8.460100   0.307852   0.220079   0.215649\nSitting     5.57050     1.75055    8.417200   0.065783   0.039344   0.058595\nSitting     5.56870     1.75090    8.403750   0.100082   0.030311   0.047059\nSitting     5.59700     1.77430    8.383800   0.025500   0.022518   0.035889\n...             ...         ...         ...        ...        ...        ...\nJogging    -4.69425    -1.78755   -0.474737   9.084436   6.577406   6.362226\nJogging    -4.81880    -2.00750   -0.689939   8.496293   6.696269   5.883700\nJogging    -4.89805    -2.23775   -0.870272   8.522679   7.136361   6.167295\nJogging    -4.37000    -2.57760   -0.800604   8.537802   6.274359   5.790770\nJogging    -4.78610    -1.81925   -0.106988   8.351498   6.374422   5.290954\n\n[4345 rows x 6 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x-axismean</th>\n      <th>y-axismean</th>\n      <th>z-axismean</th>\n      <th>x-axisstd</th>\n      <th>y-axisstd</th>\n      <th>z-axisstd</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Sitting</th>\n      <td>5.58130</td>\n      <td>1.89995</td>\n      <td>8.344250</td>\n      <td>0.121797</td>\n      <td>0.096139</td>\n      <td>0.104542</td>\n    </tr>\n    <tr>\n      <th>Sitting</th>\n      <td>5.47165</td>\n      <td>1.72050</td>\n      <td>8.460100</td>\n      <td>0.307852</td>\n      <td>0.220079</td>\n      <td>0.215649</td>\n    </tr>\n    <tr>\n      <th>Sitting</th>\n      <td>5.57050</td>\n      <td>1.75055</td>\n      <td>8.417200</td>\n      <td>0.065783</td>\n      <td>0.039344</td>\n      <td>0.058595</td>\n    </tr>\n    <tr>\n      <th>Sitting</th>\n      <td>5.56870</td>\n      <td>1.75090</td>\n      <td>8.403750</td>\n      <td>0.100082</td>\n      <td>0.030311</td>\n      <td>0.047059</td>\n    </tr>\n    <tr>\n      <th>Sitting</th>\n      <td>5.59700</td>\n      <td>1.77430</td>\n      <td>8.383800</td>\n      <td>0.025500</td>\n      <td>0.022518</td>\n      <td>0.035889</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>Jogging</th>\n      <td>-4.69425</td>\n      <td>-1.78755</td>\n      <td>-0.474737</td>\n      <td>9.084436</td>\n      <td>6.577406</td>\n      <td>6.362226</td>\n    </tr>\n    <tr>\n      <th>Jogging</th>\n      <td>-4.81880</td>\n      <td>-2.00750</td>\n      <td>-0.689939</td>\n      <td>8.496293</td>\n      <td>6.696269</td>\n      <td>5.883700</td>\n    </tr>\n    <tr>\n      <th>Jogging</th>\n      <td>-4.89805</td>\n      <td>-2.23775</td>\n      <td>-0.870272</td>\n      <td>8.522679</td>\n      <td>7.136361</td>\n      <td>6.167295</td>\n    </tr>\n    <tr>\n      <th>Jogging</th>\n      <td>-4.37000</td>\n      <td>-2.57760</td>\n      <td>-0.800604</td>\n      <td>8.537802</td>\n      <td>6.274359</td>\n      <td>5.790770</td>\n    </tr>\n    <tr>\n      <th>Jogging</th>\n      <td>-4.78610</td>\n      <td>-1.81925</td>\n      <td>-0.106988</td>\n      <td>8.351498</td>\n      <td>6.374422</td>\n      <td>5.290954</td>\n    </tr>\n  </tbody>\n</table>\n<p>4345 rows × 6 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#train_df['activity'] = train_df.index","metadata":{"execution":{"iopub.status.busy":"2024-07-12T09:31:58.199154Z","iopub.execute_input":"2024-07-12T09:31:58.199659Z","iopub.status.idle":"2024-07-12T09:31:58.204465Z","shell.execute_reply.started":"2024-07-12T09:31:58.199619Z","shell.execute_reply":"2024-07-12T09:31:58.203557Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#train_df.reset_index(drop=True, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T09:31:58.205676Z","iopub.execute_input":"2024-07-12T09:31:58.205953Z","iopub.status.idle":"2024-07-12T09:31:58.212257Z","shell.execute_reply.started":"2024-07-12T09:31:58.205929Z","shell.execute_reply":"2024-07-12T09:31:58.211317Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#train_df","metadata":{"execution":{"iopub.status.busy":"2024-07-12T09:31:58.214407Z","iopub.execute_input":"2024-07-12T09:31:58.214776Z","iopub.status.idle":"2024-07-12T09:31:58.231114Z","shell.execute_reply.started":"2024-07-12T09:31:58.214738Z","shell.execute_reply":"2024-07-12T09:31:58.230180Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"      x-axismean  y-axismean  z-axismean  x-axisstd  y-axisstd  z-axisstd  \\\n0        5.58130     1.89995    8.344250   0.121797   0.096139   0.104542   \n1        5.47165     1.72050    8.460100   0.307852   0.220079   0.215649   \n2        5.57050     1.75055    8.417200   0.065783   0.039344   0.058595   \n3        5.56870     1.75090    8.403750   0.100082   0.030311   0.047059   \n4        5.59700     1.77430    8.383800   0.025500   0.022518   0.035889   \n...          ...         ...         ...        ...        ...        ...   \n4340    -4.69425    -1.78755   -0.474737   9.084436   6.577406   6.362226   \n4341    -4.81880    -2.00750   -0.689939   8.496293   6.696269   5.883700   \n4342    -4.89805    -2.23775   -0.870272   8.522679   7.136361   6.167295   \n4343    -4.37000    -2.57760   -0.800604   8.537802   6.274359   5.790770   \n4344    -4.78610    -1.81925   -0.106988   8.351498   6.374422   5.290954   \n\n     activity  \n0     Sitting  \n1     Sitting  \n2     Sitting  \n3     Sitting  \n4     Sitting  \n...       ...  \n4340  Jogging  \n4341  Jogging  \n4342  Jogging  \n4343  Jogging  \n4344  Jogging  \n\n[4345 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x-axismean</th>\n      <th>y-axismean</th>\n      <th>z-axismean</th>\n      <th>x-axisstd</th>\n      <th>y-axisstd</th>\n      <th>z-axisstd</th>\n      <th>activity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.58130</td>\n      <td>1.89995</td>\n      <td>8.344250</td>\n      <td>0.121797</td>\n      <td>0.096139</td>\n      <td>0.104542</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5.47165</td>\n      <td>1.72050</td>\n      <td>8.460100</td>\n      <td>0.307852</td>\n      <td>0.220079</td>\n      <td>0.215649</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5.57050</td>\n      <td>1.75055</td>\n      <td>8.417200</td>\n      <td>0.065783</td>\n      <td>0.039344</td>\n      <td>0.058595</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5.56870</td>\n      <td>1.75090</td>\n      <td>8.403750</td>\n      <td>0.100082</td>\n      <td>0.030311</td>\n      <td>0.047059</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.59700</td>\n      <td>1.77430</td>\n      <td>8.383800</td>\n      <td>0.025500</td>\n      <td>0.022518</td>\n      <td>0.035889</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4340</th>\n      <td>-4.69425</td>\n      <td>-1.78755</td>\n      <td>-0.474737</td>\n      <td>9.084436</td>\n      <td>6.577406</td>\n      <td>6.362226</td>\n      <td>Jogging</td>\n    </tr>\n    <tr>\n      <th>4341</th>\n      <td>-4.81880</td>\n      <td>-2.00750</td>\n      <td>-0.689939</td>\n      <td>8.496293</td>\n      <td>6.696269</td>\n      <td>5.883700</td>\n      <td>Jogging</td>\n    </tr>\n    <tr>\n      <th>4342</th>\n      <td>-4.89805</td>\n      <td>-2.23775</td>\n      <td>-0.870272</td>\n      <td>8.522679</td>\n      <td>7.136361</td>\n      <td>6.167295</td>\n      <td>Jogging</td>\n    </tr>\n    <tr>\n      <th>4343</th>\n      <td>-4.37000</td>\n      <td>-2.57760</td>\n      <td>-0.800604</td>\n      <td>8.537802</td>\n      <td>6.274359</td>\n      <td>5.790770</td>\n      <td>Jogging</td>\n    </tr>\n    <tr>\n      <th>4344</th>\n      <td>-4.78610</td>\n      <td>-1.81925</td>\n      <td>-0.106988</td>\n      <td>8.351498</td>\n      <td>6.374422</td>\n      <td>5.290954</td>\n      <td>Jogging</td>\n    </tr>\n  </tbody>\n</table>\n<p>4345 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**KAN**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset\nimport math\nimport pandas as pd\n\n# Define your custom neural network classes\n\nclass KANLinear(torch.nn.Module):\n    def __init__(\n        self,\n        in_features,\n        out_features,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        enable_standalone_scale_spline=True,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KANLinear, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        h = (grid_range[1] - grid_range[0]) / grid_size\n        grid = (\n            (\n                torch.arange(-spline_order, grid_size + spline_order + 1) * h\n                + grid_range[0]\n            )\n            .expand(in_features, -1)\n            .contiguous()\n        )\n        self.register_buffer(\"grid\", grid)\n\n        self.base_weight = torch.nn.Parameter(torch.Tensor(out_features, in_features))\n        self.spline_weight = torch.nn.Parameter(\n            torch.Tensor(out_features, in_features, grid_size + spline_order)\n        )\n        if enable_standalone_scale_spline:\n            self.spline_scaler = torch.nn.Parameter(\n                torch.Tensor(out_features, in_features)\n            )\n\n        self.scale_noise = scale_noise\n        self.scale_base = scale_base\n        self.scale_spline = scale_spline\n        self.enable_standalone_scale_spline = enable_standalone_scale_spline\n        self.base_activation = base_activation()\n        self.grid_eps = grid_eps\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.kaiming_uniform_(self.base_weight, a=math.sqrt(5) * self.scale_base)\n        with torch.no_grad():\n            noise = (\n                (\n                    torch.rand(self.grid_size + 1, self.in_features, self.out_features)\n                    - 1 / 2\n                )\n                * self.scale_noise\n                / self.grid_size\n            )\n            self.spline_weight.data.copy_(\n                (self.scale_spline if not self.enable_standalone_scale_spline else 1.0)\n                * self.curve2coeff(\n                    self.grid.T[self.spline_order : -self.spline_order],\n                    noise,\n                )\n            )\n            if self.enable_standalone_scale_spline:\n                # torch.nn.init.constant_(self.spline_scaler, self.scale_spline)\n                torch.nn.init.kaiming_uniform_(self.spline_scaler, a=math.sqrt(5) * self.scale_spline)\n\n    def b_splines(self, x: torch.Tensor):\n        \"\"\"\n        Compute the B-spline bases for the given input tensor.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n\n        Returns:\n            torch.Tensor: B-spline bases tensor of shape (batch_size, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n\n        grid: torch.Tensor = (\n            self.grid\n        )  # (in_features, grid_size + 2 * spline_order + 1)\n        x = x.unsqueeze(-1)\n        bases = ((x >= grid[:, :-1]) & (x < grid[:, 1:])).to(x.dtype)\n        for k in range(1, self.spline_order + 1):\n            bases = (\n                (x - grid[:, : -(k + 1)])\n                / (grid[:, k:-1] - grid[:, : -(k + 1)])\n                * bases[:, :, :-1]\n            ) + (\n                (grid[:, k + 1 :] - x)\n                / (grid[:, k + 1 :] - grid[:, 1:(-k)])\n                * bases[:, :, 1:]\n            )\n\n        assert bases.size() == (\n            x.size(0),\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return bases.contiguous()\n\n    def curve2coeff(self, x: torch.Tensor, y: torch.Tensor):\n        \"\"\"\n        Compute the coefficients of the curve that interpolates the given points.\n\n        Args:\n            x (torch.Tensor): Input tensor of shape (batch_size, in_features).\n            y (torch.Tensor): Output tensor of shape (batch_size, in_features, out_features).\n\n        Returns:\n            torch.Tensor: Coefficients tensor of shape (out_features, in_features, grid_size + spline_order).\n        \"\"\"\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        assert y.size() == (x.size(0), self.in_features, self.out_features)\n\n        A = self.b_splines(x).transpose(\n            0, 1\n        )  # (in_features, batch_size, grid_size + spline_order)\n        B = y.transpose(0, 1)  # (in_features, batch_size, out_features)\n        solution = torch.linalg.lstsq(\n            A, B\n        ).solution  # (in_features, grid_size + spline_order, out_features)\n        result = solution.permute(\n            2, 0, 1\n        )  # (out_features, in_features, grid_size + spline_order)\n\n        assert result.size() == (\n            self.out_features,\n            self.in_features,\n            self.grid_size + self.spline_order,\n        )\n        return result.contiguous()\n\n    @property\n    def scaled_spline_weight(self):\n        return self.spline_weight * (\n            self.spline_scaler.unsqueeze(-1)\n            if self.enable_standalone_scale_spline\n            else 1.0\n        )\n\n    def forward(self, x: torch.Tensor):\n        assert x.size(-1) == self.in_features\n        original_shape = x.shape\n        x = x.view(-1, self.in_features)\n\n        base_output = F.linear(self.base_activation(x), self.base_weight)\n        spline_output = F.linear(\n            self.b_splines(x).view(x.size(0), -1),\n            self.scaled_spline_weight.view(self.out_features, -1),\n        )\n        output = base_output + spline_output\n        \n        output = output.view(*original_shape[:-1], self.out_features)\n        return output\n\n    @torch.no_grad()\n    def update_grid(self, x: torch.Tensor, margin=0.01):\n        assert x.dim() == 2 and x.size(1) == self.in_features\n        batch = x.size(0)\n\n        splines = self.b_splines(x)  # (batch, in, coeff)\n        splines = splines.permute(1, 0, 2)  # (in, batch, coeff)\n        orig_coeff = self.scaled_spline_weight  # (out, in, coeff)\n        orig_coeff = orig_coeff.permute(1, 2, 0)  # (in, coeff, out)\n        unreduced_spline_output = torch.bmm(splines, orig_coeff)  # (in, batch, out)\n        unreduced_spline_output = unreduced_spline_output.permute(\n            1, 0, 2\n        )  # (batch, in, out)\n\n        # sort each channel individually to collect data distribution\n        x_sorted = torch.sort(x, dim=0)[0]\n        grid_adaptive = x_sorted[\n            torch.linspace(\n                0, batch - 1, self.grid_size + 1, dtype=torch.int64, device=x.device\n            )\n        ]\n\n        uniform_step = (x_sorted[-1] - x_sorted[0] + 2 * margin) / self.grid_size\n        grid_uniform = (\n            torch.arange(\n                self.grid_size + 1, dtype=torch.float32, device=x.device\n            ).unsqueeze(1)\n            * uniform_step\n            + x_sorted[0]\n            - margin\n        )\n\n        grid = self.grid_eps * grid_uniform + (1 - self.grid_eps) * grid_adaptive\n        grid = torch.cat(\n            [\n                grid[:1]\n                - uniform_step\n                * torch.arange(self.spline_order, 0, -1, device=x.device).unsqueeze(1),\n                grid,\n                grid[-1:]\n                + uniform_step\n                * torch.arange(1, self.spline_order + 1, device=x.device).unsqueeze(1),\n            ],\n            dim=0,\n        )\n\n        self.grid.copy_(grid.T)\n        self.spline_weight.data.copy_(self.curve2coeff(x, unreduced_spline_output))\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        \"\"\"\n        Compute the regularization loss.\n\n        This is a dumb simulation of the original L1 regularization as stated in the\n        paper, since the original one requires computing absolutes and entropy from the\n        expanded (batch, in_features, out_features) intermediate tensor, which is hidden\n        behind the F.linear function if we want an memory efficient implementation.\n\n        The L1 regularization is now computed as mean absolute value of the spline\n        weights. The authors implementation also includes this term in addition to the\n        sample-based regularization.\n        \"\"\"\n        l1_fake = self.spline_weight.abs().mean(-1)\n        regularization_loss_activation = l1_fake.sum()\n        p = l1_fake / regularization_loss_activation\n        regularization_loss_entropy = -torch.sum(p * p.log())\n        return (\n            regularize_activation * regularization_loss_activation\n            + regularize_entropy * regularization_loss_entropy\n        )\n\n\nclass KAN(torch.nn.Module):\n    def __init__(\n        self,\n        layers_hidden,\n        grid_size=5,\n        spline_order=3,\n        scale_noise=0.1,\n        scale_base=1.0,\n        scale_spline=1.0,\n        base_activation=torch.nn.SiLU,\n        grid_eps=0.02,\n        grid_range=[-1, 1],\n    ):\n        super(KAN, self).__init__()\n        self.grid_size = grid_size\n        self.spline_order = spline_order\n\n        self.layers = torch.nn.ModuleList()\n        for in_features, out_features in zip(layers_hidden, layers_hidden[1:]):\n            self.layers.append(\n                KANLinear(\n                    in_features,\n                    out_features,\n                    grid_size=grid_size,\n                    spline_order=spline_order,\n                    scale_noise=scale_noise,\n                    scale_base=scale_base,\n                    scale_spline=scale_spline,\n                    base_activation=base_activation,\n                    grid_eps=grid_eps,\n                    grid_range=grid_range,\n                )\n            )\n\n    def forward(self, x: torch.Tensor, update_grid=False):\n        for layer in self.layers:\n            if update_grid:\n                layer.update_grid(x)\n            x = layer(x)\n        return x\n\n    def regularization_loss(self, regularize_activation=1.0, regularize_entropy=1.0):\n        return sum(\n            layer.regularization_loss(regularize_activation, regularize_entropy)\n            for layer in self.layers\n        )","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:30:47.682591Z","iopub.execute_input":"2024-07-12T12:30:47.682990Z","iopub.status.idle":"2024-07-12T12:30:47.725856Z","shell.execute_reply.started":"2024-07-12T12:30:47.682961Z","shell.execute_reply":"2024-07-12T12:30:47.724848Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# Split the dataset into train and validation sets\ntrain_df, val_df = train_test_split(combined_df, test_size=0.2, random_state=42,shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:36:31.924907Z","iopub.execute_input":"2024-07-12T12:36:31.925280Z","iopub.status.idle":"2024-07-12T12:36:31.932273Z","shell.execute_reply.started":"2024-07-12T12:36:31.925252Z","shell.execute_reply":"2024-07-12T12:36:31.931317Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"X_train = torch.tensor(train_df[['x-axismean', 'y-axismean', 'z-axismean', \n                                 'x-axisstd', 'y-axisstd', 'z-axisstd']].values, dtype=torch.float32)\ny_train = torch.tensor(train_df['label'].astype('category').cat.codes.values, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:36:32.875318Z","iopub.execute_input":"2024-07-12T12:36:32.875738Z","iopub.status.idle":"2024-07-12T12:36:32.884406Z","shell.execute_reply.started":"2024-07-12T12:36:32.875705Z","shell.execute_reply":"2024-07-12T12:36:32.883323Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"# Prepare validation data\nX_val = torch.tensor(val_df[['x-axismean', 'y-axismean', 'z-axismean', \n                             'x-axisstd', 'y-axisstd', 'z-axisstd']].values, dtype=torch.float32)\ny_val = torch.tensor(val_df['label'].astype('category').cat.codes.values, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:36:33.107326Z","iopub.execute_input":"2024-07-12T12:36:33.107699Z","iopub.status.idle":"2024-07-12T12:36:33.115608Z","shell.execute_reply.started":"2024-07-12T12:36:33.107671Z","shell.execute_reply":"2024-07-12T12:36:33.114462Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"# Define your model architecture\nlayers_hidden = [X_train.shape[1], 64, 32, len(train_df['label'].unique())]  # Adjust hidden layers as needed\nmodel = KAN(layers_hidden)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:36:33.337144Z","iopub.execute_input":"2024-07-12T12:36:33.337463Z","iopub.status.idle":"2024-07-12T12:36:33.349269Z","shell.execute_reply.started":"2024-07-12T12:36:33.337437Z","shell.execute_reply":"2024-07-12T12:36:33.348438Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Create datasets and dataloaders for training and validation\nbatch_size = 128\ntrain_dataset = TensorDataset(X_train, y_train)\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\nval_dataset = TensorDataset(X_val, y_val)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:36:33.581784Z","iopub.execute_input":"2024-07-12T12:36:33.582112Z","iopub.status.idle":"2024-07-12T12:36:33.588846Z","shell.execute_reply.started":"2024-07-12T12:36:33.582088Z","shell.execute_reply":"2024-07-12T12:36:33.587823Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"# Training loop with validation\nnum_epochs = 10\nfor epoch in range(num_epochs):\n    # Training\n    model.train()\n    train_loss = 0.0\n    correct_train = 0\n    total_train = 0\n    for inputs, targets in train_dataloader:\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item() * inputs.size(0)\n        _, predicted_train = torch.max(outputs, 1)\n        total_train += targets.size(0)\n        correct_train += (predicted_train == targets).sum().item()\n    \n    train_loss /= len(train_dataloader.dataset)\n    train_accuracy = correct_train / total_train\n    \n    # Validation\n    model.eval()\n    val_loss = 0.0\n    correct_val = 0\n    total_val = 0\n    with torch.no_grad():\n        for inputs, targets in val_dataloader:\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            val_loss += loss.item() * inputs.size(0)\n            _, predicted_val = torch.max(outputs, 1)\n            total_val += targets.size(0)\n            correct_val += (predicted_val == targets).sum().item()\n    \n    val_loss /= len(val_dataloader.dataset)\n    val_accuracy = correct_val / total_val\n    \n    print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-07-12T12:36:34.015141Z","iopub.execute_input":"2024-07-12T12:36:34.015901Z","iopub.status.idle":"2024-07-12T12:36:34.163599Z","shell.execute_reply.started":"2024-07-12T12:36:34.015869Z","shell.execute_reply":"2024-07-12T12:36:34.162600Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Epoch [1/10], Train Loss: 1.6516, Train Accuracy: 0.2019, Val Loss: 1.5954, Val Accuracy: 0.2308\nEpoch [2/10], Train Loss: 1.6352, Train Accuracy: 0.2019, Val Loss: 1.5889, Val Accuracy: 0.1923\nEpoch [3/10], Train Loss: 1.6206, Train Accuracy: 0.2212, Val Loss: 1.5838, Val Accuracy: 0.1923\nEpoch [4/10], Train Loss: 1.6075, Train Accuracy: 0.2404, Val Loss: 1.5798, Val Accuracy: 0.1923\nEpoch [5/10], Train Loss: 1.5957, Train Accuracy: 0.2596, Val Loss: 1.5769, Val Accuracy: 0.1538\nEpoch [6/10], Train Loss: 1.5850, Train Accuracy: 0.2981, Val Loss: 1.5748, Val Accuracy: 0.1538\nEpoch [7/10], Train Loss: 1.5752, Train Accuracy: 0.3269, Val Loss: 1.5733, Val Accuracy: 0.1154\nEpoch [8/10], Train Loss: 1.5661, Train Accuracy: 0.3462, Val Loss: 1.5722, Val Accuracy: 0.1154\nEpoch [9/10], Train Loss: 1.5575, Train Accuracy: 0.3558, Val Loss: 1.5712, Val Accuracy: 0.1538\nEpoch [10/10], Train Loss: 1.5493, Train Accuracy: 0.3558, Val Loss: 1.5703, Val Accuracy: 0.1538\n","output_type":"stream"}]},{"cell_type":"code","source":"# Optionally, save the model\ntorch.save(model.state_dict(), 'kan_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:33:08.037596Z","iopub.execute_input":"2024-07-12T12:33:08.037992Z","iopub.status.idle":"2024-07-12T12:33:08.046060Z","shell.execute_reply.started":"2024-07-12T12:33:08.037965Z","shell.execute_reply":"2024-07-12T12:33:08.045098Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"---","metadata":{}},{"cell_type":"markdown","source":"**PREPROCESS**","metadata":{}},{"cell_type":"code","source":"import os\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:01:12.102070Z","iopub.execute_input":"2024-07-12T12:01:12.102433Z","iopub.status.idle":"2024-07-12T12:01:12.466729Z","shell.execute_reply.started":"2024-07-12T12:01:12.102405Z","shell.execute_reply":"2024-07-12T12:01:12.465831Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Downstairs'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\ndownstairscombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\ndownstairscombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:37.099949Z","iopub.execute_input":"2024-07-12T12:25:37.100301Z","iopub.status.idle":"2024-07-12T12:25:37.245188Z","shell.execute_reply.started":"2024-07-12T12:25:37.100275Z","shell.execute_reply":"2024-07-12T12:25:37.244406Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Jogging'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\njogingcombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\njogingcombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:37.354733Z","iopub.execute_input":"2024-07-12T12:25:37.355540Z","iopub.status.idle":"2024-07-12T12:25:37.646860Z","shell.execute_reply.started":"2024-07-12T12:25:37.355507Z","shell.execute_reply":"2024-07-12T12:25:37.645982Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Jogging'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\nsittingcombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\nsittingcombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:37.648442Z","iopub.execute_input":"2024-07-12T12:25:37.648767Z","iopub.status.idle":"2024-07-12T12:25:37.933279Z","shell.execute_reply.started":"2024-07-12T12:25:37.648741Z","shell.execute_reply":"2024-07-12T12:25:37.932210Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Jogging'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\nstandingcombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\nstandingcombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:38.049572Z","iopub.execute_input":"2024-07-12T12:25:38.050461Z","iopub.status.idle":"2024-07-12T12:25:38.336295Z","shell.execute_reply.started":"2024-07-12T12:25:38.050428Z","shell.execute_reply":"2024-07-12T12:25:38.335515Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Jogging'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\nupstairscombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\nupstairscombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:38.689563Z","iopub.execute_input":"2024-07-12T12:25:38.690622Z","iopub.status.idle":"2024-07-12T12:25:38.981995Z","shell.execute_reply.started":"2024-07-12T12:25:38.690584Z","shell.execute_reply":"2024-07-12T12:25:38.981173Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/train/Jogging'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\nwalkingcombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\nwalkingcombined_df.set_index('filename', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:39.633512Z","iopub.execute_input":"2024-07-12T12:25:39.634409Z","iopub.status.idle":"2024-07-12T12:25:39.903654Z","shell.execute_reply.started":"2024-07-12T12:25:39.634376Z","shell.execute_reply":"2024-07-12T12:25:39.902604Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"downstairscombined_df['label'] = 'Downstairs'\njogingcombined_df['label'] = 'Joging'\nsittingcombined_df['label'] = 'Sitting'\nstandingcombined_df['label'] = 'Standing'\nupstairscombined_df['label'] = 'Upstairs'\nwalkingcombined_df['label'] = 'Walking'","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:25:40.753823Z","iopub.execute_input":"2024-07-12T12:25:40.754163Z","iopub.status.idle":"2024-07-12T12:25:40.762364Z","shell.execute_reply.started":"2024-07-12T12:25:40.754137Z","shell.execute_reply":"2024-07-12T12:25:40.761350Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"combined_df = pd.concat([sittingcombined_df, standingcombined_df, upstairscombined_df, walkingcombined_df, downstairscombined_df], ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:28:52.555023Z","iopub.execute_input":"2024-07-12T12:28:52.555362Z","iopub.status.idle":"2024-07-12T12:28:52.561279Z","shell.execute_reply.started":"2024-07-12T12:28:52.555336Z","shell.execute_reply":"2024-07-12T12:28:52.560340Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"combined_df","metadata":{"execution":{"iopub.status.busy":"2024-07-12T12:28:55.309380Z","iopub.execute_input":"2024-07-12T12:28:55.309913Z","iopub.status.idle":"2024-07-12T12:28:55.346303Z","shell.execute_reply.started":"2024-07-12T12:28:55.309875Z","shell.execute_reply":"2024-07-12T12:28:55.345274Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"     x-axismean  x-axisstd  x-axismin  x-axismedian  x-axismax  y-axismean  \\\n0     -3.660191  10.245137     -19.61         -3.68      19.57    8.919840   \n1      4.860125   9.837901     -19.23          2.79      19.57    6.248177   \n2     -0.208456  10.706282     -19.61         -0.04      19.57    0.197931   \n3     -2.076025   9.702477     -19.57         -2.72      19.61   -5.490687   \n4     -2.588529   4.705870     -19.46         -2.11      15.09    8.049402   \n..          ...        ...        ...           ...        ...         ...   \n125   -3.641629   3.525674     -17.86         -3.64       6.44    8.897423   \n126    2.156029   2.716000      -6.32          1.80      14.44    9.524473   \n127   -0.653679   3.574959     -14.71         -0.80      15.94   10.020266   \n128    1.117113   2.715086     -10.46          0.99      13.29    8.275815   \n129   -2.158006   4.282436     -19.61         -1.65      16.97    8.746916   \n\n     y-axisstd  y-axismin  y-axismedian  y-axismax  z-axismean  z-axisstd  \\\n0     8.329447     -15.94         10.69      19.57   -0.100199   5.488763   \n1     8.370642     -10.23          2.79      19.57   -0.174064   4.568252   \n2     9.483499     -19.61          0.42      19.57    1.164557   5.653878   \n3    11.270352     -19.61         -6.55      19.57    0.844796   5.730873   \n4     6.987964      -7.63          7.55      19.57   -2.207300   3.470223   \n..         ...        ...           ...        ...         ...        ...   \n125   4.120945      -1.23          8.47      19.57   -0.091903   2.884186   \n126   3.768547      -0.27          9.58      19.57   -0.037363   2.129105   \n127   4.193161      -2.22          9.70      19.57    1.134540   4.211064   \n128   4.781041     -10.00          8.31      19.93    2.904096   2.689611   \n129   5.264788      -8.73          8.05      19.57    1.175538   4.328685   \n\n     z-axismin  z-axismedian  z-axismax       label  \n0   -16.739407      0.000000  18.619015     Sitting  \n1   -14.137921     -0.081722  16.889230     Sitting  \n2   -17.200000      0.570000  19.000000     Sitting  \n3   -19.570000      0.990000  19.610000     Sitting  \n4   -17.920000     -1.570000  15.210000     Sitting  \n..         ...           ...        ...         ...  \n125 -12.299174     -0.531194  14.601013  Downstairs  \n126  -6.701211     -0.340509  11.645397  Downstairs  \n127 -13.210000      0.380000  18.240000  Downstairs  \n128  -6.050000      2.600000  13.570000  Downstairs  \n129 -12.980191      0.340509  17.692831  Downstairs  \n\n[130 rows x 16 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>x-axismean</th>\n      <th>x-axisstd</th>\n      <th>x-axismin</th>\n      <th>x-axismedian</th>\n      <th>x-axismax</th>\n      <th>y-axismean</th>\n      <th>y-axisstd</th>\n      <th>y-axismin</th>\n      <th>y-axismedian</th>\n      <th>y-axismax</th>\n      <th>z-axismean</th>\n      <th>z-axisstd</th>\n      <th>z-axismin</th>\n      <th>z-axismedian</th>\n      <th>z-axismax</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-3.660191</td>\n      <td>10.245137</td>\n      <td>-19.61</td>\n      <td>-3.68</td>\n      <td>19.57</td>\n      <td>8.919840</td>\n      <td>8.329447</td>\n      <td>-15.94</td>\n      <td>10.69</td>\n      <td>19.57</td>\n      <td>-0.100199</td>\n      <td>5.488763</td>\n      <td>-16.739407</td>\n      <td>0.000000</td>\n      <td>18.619015</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.860125</td>\n      <td>9.837901</td>\n      <td>-19.23</td>\n      <td>2.79</td>\n      <td>19.57</td>\n      <td>6.248177</td>\n      <td>8.370642</td>\n      <td>-10.23</td>\n      <td>2.79</td>\n      <td>19.57</td>\n      <td>-0.174064</td>\n      <td>4.568252</td>\n      <td>-14.137921</td>\n      <td>-0.081722</td>\n      <td>16.889230</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.208456</td>\n      <td>10.706282</td>\n      <td>-19.61</td>\n      <td>-0.04</td>\n      <td>19.57</td>\n      <td>0.197931</td>\n      <td>9.483499</td>\n      <td>-19.61</td>\n      <td>0.42</td>\n      <td>19.57</td>\n      <td>1.164557</td>\n      <td>5.653878</td>\n      <td>-17.200000</td>\n      <td>0.570000</td>\n      <td>19.000000</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-2.076025</td>\n      <td>9.702477</td>\n      <td>-19.57</td>\n      <td>-2.72</td>\n      <td>19.61</td>\n      <td>-5.490687</td>\n      <td>11.270352</td>\n      <td>-19.61</td>\n      <td>-6.55</td>\n      <td>19.57</td>\n      <td>0.844796</td>\n      <td>5.730873</td>\n      <td>-19.570000</td>\n      <td>0.990000</td>\n      <td>19.610000</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-2.588529</td>\n      <td>4.705870</td>\n      <td>-19.46</td>\n      <td>-2.11</td>\n      <td>15.09</td>\n      <td>8.049402</td>\n      <td>6.987964</td>\n      <td>-7.63</td>\n      <td>7.55</td>\n      <td>19.57</td>\n      <td>-2.207300</td>\n      <td>3.470223</td>\n      <td>-17.920000</td>\n      <td>-1.570000</td>\n      <td>15.210000</td>\n      <td>Sitting</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125</th>\n      <td>-3.641629</td>\n      <td>3.525674</td>\n      <td>-17.86</td>\n      <td>-3.64</td>\n      <td>6.44</td>\n      <td>8.897423</td>\n      <td>4.120945</td>\n      <td>-1.23</td>\n      <td>8.47</td>\n      <td>19.57</td>\n      <td>-0.091903</td>\n      <td>2.884186</td>\n      <td>-12.299174</td>\n      <td>-0.531194</td>\n      <td>14.601013</td>\n      <td>Downstairs</td>\n    </tr>\n    <tr>\n      <th>126</th>\n      <td>2.156029</td>\n      <td>2.716000</td>\n      <td>-6.32</td>\n      <td>1.80</td>\n      <td>14.44</td>\n      <td>9.524473</td>\n      <td>3.768547</td>\n      <td>-0.27</td>\n      <td>9.58</td>\n      <td>19.57</td>\n      <td>-0.037363</td>\n      <td>2.129105</td>\n      <td>-6.701211</td>\n      <td>-0.340509</td>\n      <td>11.645397</td>\n      <td>Downstairs</td>\n    </tr>\n    <tr>\n      <th>127</th>\n      <td>-0.653679</td>\n      <td>3.574959</td>\n      <td>-14.71</td>\n      <td>-0.80</td>\n      <td>15.94</td>\n      <td>10.020266</td>\n      <td>4.193161</td>\n      <td>-2.22</td>\n      <td>9.70</td>\n      <td>19.57</td>\n      <td>1.134540</td>\n      <td>4.211064</td>\n      <td>-13.210000</td>\n      <td>0.380000</td>\n      <td>18.240000</td>\n      <td>Downstairs</td>\n    </tr>\n    <tr>\n      <th>128</th>\n      <td>1.117113</td>\n      <td>2.715086</td>\n      <td>-10.46</td>\n      <td>0.99</td>\n      <td>13.29</td>\n      <td>8.275815</td>\n      <td>4.781041</td>\n      <td>-10.00</td>\n      <td>8.31</td>\n      <td>19.93</td>\n      <td>2.904096</td>\n      <td>2.689611</td>\n      <td>-6.050000</td>\n      <td>2.600000</td>\n      <td>13.570000</td>\n      <td>Downstairs</td>\n    </tr>\n    <tr>\n      <th>129</th>\n      <td>-2.158006</td>\n      <td>4.282436</td>\n      <td>-19.61</td>\n      <td>-1.65</td>\n      <td>16.97</td>\n      <td>8.746916</td>\n      <td>5.264788</td>\n      <td>-8.73</td>\n      <td>8.05</td>\n      <td>19.57</td>\n      <td>1.175538</td>\n      <td>4.328685</td>\n      <td>-12.980191</td>\n      <td>0.340509</td>\n      <td>17.692831</td>\n      <td>Downstairs</td>\n    </tr>\n  </tbody>\n</table>\n<p>130 rows × 16 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Directory containing the CSV files\ninput_dir = '/kaggle/input/human-activity-recognition-har/HAR/test'\n\n# List to store the combined statistics for all files\ncombined_stats_list = []\n\n# Walk through the directory and read all CSV files\nfor dirname, _, filenames in os.walk(input_dir):\n    for filename in filenames:\n        if filename.endswith('.csv'):\n            file_path = os.path.join(dirname, filename)\n            df = pd.read_csv(file_path)\n            #print(f\"Loaded {filename}\")\n\n            # Calculate the mean and standard deviation of each column\n            mean_values = df.mean()\n            std_values = df.std()\n            min_values = df.min()\n            median_values = df.median()\n            max_values = df.max()\n\n            # Create a new dictionary to store combined mean and standard deviation\n            combined_stats = {'filename': filename}\n\n            # Combine mean and std into the new dictionary\n            for col in mean_values.index:\n                combined_stats[col + 'mean'] = mean_values[col]\n                combined_stats[col + 'std'] = std_values[col]\n                combined_stats[col + 'min'] = min_values[col]\n                combined_stats[col + 'median'] = median_values[col]\n                combined_stats[col + 'max'] = max_values[col]\n\n            # Append the combined stats dictionary to the list\n            combined_stats_list.append(combined_stats)\n\n# Convert the list of dictionaries to a DataFrame\ntestcombined_df = pd.DataFrame(combined_stats_list)\n\n# Set the filename as the index\ntestcombined_df.set_index('filename', inplace=True)","metadata":{},"execution_count":null,"outputs":[]}]}